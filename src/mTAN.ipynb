{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mTAN - Imputation Model\n",
    "\n",
    "Here we will test out the mTAN imputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '../../toy_dataset/'\n",
    "sys.path.append(path)\n",
    "\n",
    "from data_utils import ToyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '/home2/joshua.wendland/Documents/sepsis/toy_dataset/synthetic_ts_1/synthetic_ts_test_data_eav.csv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import, Import, Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'physionet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrandom\u001b[39;00m \u001b[39mimport\u001b[39;00m SystemRandom\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/sepsis/imputation/mTAN/src/utils.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphysionet\u001b[39;00m \u001b[39mimport\u001b[39;00m PhysioNet, get_data_min_max, variable_time_collate_fn2\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m model_selection\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'physionet'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "from random import SystemRandom\n",
    "import src.models\n",
    "import src.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--niters', type=int, default=2000, help='Number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--std', type=float, default=0.01)\n",
    "parser.add_argument('--latent-dim', type=int, default=32)\n",
    "parser.add_argument('--rec-hidden', type=int, default=32)\n",
    "parser.add_argument('--gen-hidden', type=int, default=50)\n",
    "parser.add_argument('--embed-time', type=int, default=128)\n",
    "parser.add_argument('--k-iwae', type=int, default=10)\n",
    "parser.add_argument('--save', type=int, default=1)\n",
    "parser.add_argument('--enc', type=str, default='mtan_rnn')\n",
    "parser.add_argument('--dec', type=str, default='mtan_rnn')\n",
    "parser.add_argument('--fname', type=str, default=None)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--n', type=int, default=8000)\n",
    "parser.add_argument('--batch-size', type=int, default=50)\n",
    "parser.add_argument('--quantization', type=float, default=0.016,\n",
    "                    help=\"Quantization on the physionet dataset.\")\n",
    "parser.add_argument('--classif', action='store_true',\n",
    "                    help=\"Include binary classification loss\")\n",
    "parser.add_argument('--norm', action='store_true')\n",
    "parser.add_argument('--kl', action='store_true')\n",
    "parser.add_argument('--learn-emb', action='store_true')\n",
    "parser.add_argument('--enc-num-heads', type=int, default=1)\n",
    "parser.add_argument('--dec-num-heads', type=int, default=1)\n",
    "parser.add_argument('--length', type=int, default=20)\n",
    "parser.add_argument('--num-ref-points', type=int, default=128)\n",
    "parser.add_argument('--dataset', type=str, default='toy')\n",
    "parser.add_argument('--enc-rnn', action='store_false')\n",
    "parser.add_argument('--dec-rnn', action='store_false')\n",
    "parser.add_argument('--sample-tp', type=float, default=1.0)\n",
    "parser.add_argument('--only-periodic', type=str, default=None)\n",
    "parser.add_argument('--dropout', type=float, default=0.0)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "experiment_id = int(SystemRandom().random() * 100000)\n",
    "print(args, experiment_id)\n",
    "seed = args.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Set the device to run calculation on\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# consider the correct dataset\n",
    "if args.dataset == 'toy':\n",
    "    data_obj = utils.kernel_smoother_data_gen(args, alpha=100., seed=0)\n",
    "\n",
    "train_loader = data_obj[\"train_dataloader\"]\n",
    "test_loader = data_obj[\"test_dataloader\"]\n",
    "dim = data_obj[\"input_dim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.enc == 'enc_rnn3':\n",
    "    rec = models.enc_rnn3(\n",
    "        dim, torch.linspace(0, 1., args.num_ref_points), args.latent_dim, \n",
    "        args.rec_hidden, 128, learn_emb=args.learn_emb).to(device)\n",
    "elif args.enc == 'mtan_rnn':\n",
    "    rec = models.enc_mtan_rnn(\n",
    "        dim, torch.linspace(0, 1., args.num_ref_points), args.latent_dim, args.rec_hidden, \n",
    "        embed_time=128, learn_emb=args.learn_emb, num_heads=args.enc_num_heads).to(device)\n",
    "\n",
    "    \n",
    "if args.dec == 'rnn3':\n",
    "    dec = models.dec_rnn3(\n",
    "        dim, torch.linspace(0, 1., args.num_ref_points), args.latent_dim, \n",
    "        args.gen_hidden, 128, learn_emb=args.learn_emb).to(device)\n",
    "elif args.dec == 'mtan_rnn':\n",
    "    dec = models.dec_mtan_rnn(\n",
    "        dim, torch.linspace(0, 1., args.num_ref_points), args.latent_dim, args.gen_hidden, \n",
    "        embed_time=128, learn_emb=args.learn_emb, num_heads=args.dec_num_heads).to(device)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up optimizer, tensorboard, load model weights (and more?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (list(dec.parameters()) + list(rec.parameters()))\n",
    "optimizer = optim.Adam(params, lr=args.lr)\n",
    "print('parameters:', utils.count_parameters(rec), utils.count_parameters(dec))\n",
    "if args.fname is not None:\n",
    "    checkpoint = torch.load(args.fname)\n",
    "    rec.load_state_dict(checkpoint['rec_state_dict'])\n",
    "    dec.load_state_dict(checkpoint['dec_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print('loading saved weights', checkpoint['epoch'])\n",
    "    print('Test MSE', utils.evaluate(dim, rec, dec, test_loader, args, 1))\n",
    "    print('Test MSE', utils.evaluate(dim, rec, dec, test_loader, args, 3))\n",
    "    print('Test MSE', utils.evaluate(dim, rec, dec, test_loader, args, 10))\n",
    "    print('Test MSE', utils.evaluate(dim, rec, dec, test_loader, args, 20))\n",
    "    print('Test MSE', utils.evaluate(dim, rec, dec, test_loader, args, 30))\n",
    "    print('Test MSE', utils.evaluate(dim, rec, dec, test_loader, args, 50))\n",
    "\n",
    "\n",
    "# Set up Tensorboard\n",
    "start_time = datetime.now().strftime(\"%Y.%m.%d.%H.%M.%S\")\n",
    "path = '/home2/joshua.wendland/Documents/sepsis/imputation/mTAN/runs/'\n",
    "path += f'{args.dataset}/'\n",
    "path += f'/{start_time}'\n",
    "writer = SummaryWriter(log_dir=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train through epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through epochs\n",
    "for itr in range(1, args.niters + 1):\n",
    "    train_loss = 0\n",
    "    train_n = 0\n",
    "    avg_reconst, avg_kl, mse = 0, 0, 0\n",
    "    if args.kl:\n",
    "        wait_until_kl_inc = 10\n",
    "        if itr < wait_until_kl_inc:\n",
    "            kl_coef = 0.\n",
    "        else:\n",
    "            kl_coef = (1 - 0.99 ** (itr - wait_until_kl_inc))\n",
    "    else:\n",
    "        kl_coef = 1\n",
    "\n",
    "    for train_batch in train_loader:\n",
    "        train_batch = train_batch.to(device)\n",
    "        batch_len = train_batch.shape[0]\n",
    "        observed_data = train_batch[:, :, :dim]\n",
    "        observed_mask = train_batch[:, :, dim:2 * dim]\n",
    "        observed_tp = train_batch[:, :, -1]\n",
    "        if args.sample_tp and args.sample_tp < 1:\n",
    "            subsampled_data, subsampled_tp, subsampled_mask = utils.subsample_timepoints(\n",
    "                observed_data.clone(), observed_tp.clone(), observed_mask.clone(), args.sample_tp)\n",
    "        else:\n",
    "            subsampled_data, subsampled_tp, subsampled_mask = \\\n",
    "                observed_data, observed_tp, observed_mask\n",
    "        out = rec(torch.cat((subsampled_data, subsampled_mask), 2), subsampled_tp)\n",
    "        qz0_mean = out[:, :, :args.latent_dim]\n",
    "        qz0_logvar = out[:, :, args.latent_dim:]\n",
    "        # epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "        epsilon = torch.randn(\n",
    "            args.k_iwae, qz0_mean.shape[0], qz0_mean.shape[1], qz0_mean.shape[2]\n",
    "        ).to(device)\n",
    "        z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "        z0 = z0.view(-1, qz0_mean.shape[1], qz0_mean.shape[2])\n",
    "        pred_x = dec(\n",
    "            z0,\n",
    "            observed_tp[None, :, :].repeat(args.k_iwae, 1, 1).view(-1, observed_tp.shape[1])\n",
    "        )\n",
    "        # nsample, batch, seqlen, dim\n",
    "        pred_x = pred_x.view(args.k_iwae, batch_len, pred_x.shape[1], pred_x.shape[2])\n",
    "        # compute loss\n",
    "        logpx, analytic_kl = utils.compute_losses(\n",
    "            dim, train_batch, qz0_mean, qz0_logvar, pred_x, args, device)\n",
    "        loss = -(torch.logsumexp(logpx - kl_coef * analytic_kl, dim=0).mean(0) - np.log(args.k_iwae))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch_len\n",
    "        train_n += batch_len\n",
    "        avg_reconst += torch.mean(logpx) * batch_len\n",
    "        avg_kl += torch.mean(analytic_kl) * batch_len\n",
    "        mse += utils.mean_squared_error(\n",
    "            observed_data, pred_x.mean(0), observed_mask) * batch_len\n",
    "\n",
    "    print('Iter: {}, avg elbo: {:.4f}, avg reconst: {:.4f}, avg kl: {:.4f}, mse: {:.6f}'\n",
    "        .format(itr, train_loss / train_n, -avg_reconst / train_n, avg_kl / train_n, mse / train_n))\n",
    "    writer.add_scalar('avg elbo', train_loss / train_n, itr)\n",
    "    writer.add_scalar('avg reconst', -avg_reconst / train_n, itr)\n",
    "    writer.add_scalar('avg kl', avg_kl / train_n, itr)\n",
    "    writer.add_scalar('avg mse', mse / train_n, itr)\n",
    "\n",
    "    if itr % 10 == 0:\n",
    "        mse = utils.evaluate(dim, rec, dec, test_loader, args, 1)\n",
    "        writer.add_scalar('Test MSE', mse, itr)\n",
    "        print('Test Mean Squared Error', mse)\n",
    "    if itr % 10 == 0 and args.save:\n",
    "        path_save = path + args.dataset + '_' + args.enc + '_' + args.dec + '_' + str(experiment_id) + '.h5'\n",
    "        torch.save({\n",
    "            'args': args,\n",
    "            'epoch': itr,\n",
    "            'rec_state_dict': rec.state_dict(),\n",
    "            'dec_state_dict': dec.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': -loss,\n",
    "        }, path_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('mTAN': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c0149bcba5d57c6c5fe6ec16c81707307f68ffca57204c4f3e4ca3000c08467"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
